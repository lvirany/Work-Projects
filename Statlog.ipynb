{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPJjv9qaeAKlBwOPZ/lBOzS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lvirany/Work-Projects/blob/main/Statlog.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install cirq\n",
        "!pip install ucimlrepo"
      ],
      "metadata": {
        "id": "ppQbWjk9Aa_n"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e5hmtNcCAEEj",
        "outputId": "41fa865e-3f14-4e0b-b24e-8771641a6362"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cross-Validation Accuracy Scores: [0.70625 0.6625  0.7375  0.75625 0.74375]\n",
            "Mean Cross-Validation Accuracy: 72.12%\n",
            "Train Accuracy: 81.00%\n",
            "Test Accuracy: 74.00%\n",
            "Predicted Defaults (Test): [2 2 2 1 1 2 1 2 1 1]\n",
            "Actual Defaults (Test): [2 1 1 1 1 1 1 1 1 1]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import cirq\n",
        "import gc # Import the gc module\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.decomposition import PCA\n",
        "from scipy.sparse import csr_matrix\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "from ucimlrepo import fetch_ucirepo\n",
        "import pickle\n",
        "\n",
        "# Fetch the dataset\n",
        "statlog_german_credit_data = fetch_ucirepo(id=144)\n",
        "\n",
        "# Load dataset features and targets\n",
        "X = statlog_german_credit_data.data.features  # Features as DataFrame\n",
        "y = statlog_german_credit_data.data.targets   # Targets as Series\n",
        "\n",
        "# Inspect dataset metadata\n",
        "#print(statlog_german_credit_data.metadata)\n",
        "#print(statlog_german_credit_data.variables)\n",
        "\n",
        "# Preprocess the data\n",
        "# Normalize the features to [0, 1]\n",
        "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
        "\n",
        "# Encode categorical features\n",
        "categorical_columns = ['Attribute1', 'Attribute3', 'Attribute4', 'Attribute6', 'Attribute7',\n",
        "                       'Attribute9', 'Attribute10', 'Attribute12', 'Attribute14', 'Attribute15',\n",
        "                       'Attribute17', 'Attribute19', 'Attribute20']\n",
        "\n",
        "label_encoders = {}\n",
        "for col in categorical_columns:\n",
        "    label_encoders[col] = LabelEncoder()\n",
        "    # Apply the label encoding and update the DataFrame\n",
        "    X.loc[:, col] = label_encoders[col].fit_transform(X[col])\n",
        "\n",
        "# Clear label encoders and categorical column list\n",
        "del label_encoders, categorical_columns\n",
        "gc.collect()\n",
        "\n",
        "# Normalize the features to [0, 1]\n",
        "scaler = MinMaxScaler()\n",
        "X_normalized = scaler.fit_transform(X)\n",
        "\n",
        "# Apply PCA, if necessary, to reduce the number of features\n",
        "pca = PCA(n_components=18)  # Adjust components based on available memory\n",
        "X_reduced = pca.fit_transform(X_normalized)\n",
        "\n",
        "del X, X_normalized\n",
        "import gc\n",
        "gc.collect()\n",
        "\n",
        "# Split into training and testing sets\n",
        "# Update train-test split with reduced features\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_reduced, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Clear reduced dataset after splitting\n",
        "del X_reduced\n",
        "gc.collect()\n",
        "\n",
        "# Convert the target (y) to a 1D array (addresses warnings)\n",
        "y_train = y_train['class'].to_numpy()\n",
        "y_test = y_test['class'].to_numpy()\n",
        "\n",
        "# Update the number of qubits\n",
        "num_qubits = X_train.shape[1]  # Updated to reduced feature count\n",
        "\n",
        "# Define the Quantum Reservoir\n",
        "def create_quantum_reservoir(num_qubits, depth):\n",
        "    qubits = [cirq.GridQubit(0, i) for i in range(num_qubits)]\n",
        "    circuit = cirq.Circuit()\n",
        "    for _ in range(depth):\n",
        "        for qubit in qubits:\n",
        "            circuit.append(cirq.rx(np.random.rand() * 2 * np.pi)(qubit))\n",
        "        for i in range(len(qubits) - 1):\n",
        "            circuit.append(cirq.CNOT(qubits[i], qubits[i + 1]))\n",
        "    return circuit, qubits\n",
        "\n",
        "# Initialize the quantum reservoir\n",
        "num_qubits = X_train.shape[1]  # Number of features as qubits\n",
        "reservoir_depth = 3\n",
        "quantum_circuit, quantum_qubits = create_quantum_reservoir(num_qubits, reservoir_depth)\n",
        "\n",
        "# Encode classical data into quantum states\n",
        "def encode_classical_data_optimized(data, qubits):\n",
        "    circuit = cirq.Circuit()\n",
        "    for i, value in enumerate(data[:len(qubits)]):  # Limit encoding to available qubits\n",
        "        circuit.append(cirq.rx(value * np.pi)(qubits[i]))\n",
        "    return circuit\n",
        "\n",
        "# Extract reservoir states\n",
        "def extract_reservoir_states_sparse(circuit, qubits):\n",
        "    simulator = cirq.Simulator()\n",
        "    result = simulator.simulate(circuit)\n",
        "    state_vector = csr_matrix(result.final_state_vector)  # Store as sparse matrix\n",
        "    return np.abs(state_vector.toarray()) # Take the absolute value of the state vector\n",
        "\n",
        "# Process training and testing data through the quantum reservoir\n",
        "def process_with_quantum_reservoir_in_batches(data, quantum_circuit, quantum_qubits, batch_size=100):\n",
        "    reservoir_features = []\n",
        "    for i in range(0, len(data), batch_size):\n",
        "        batch = data[i:i + batch_size]\n",
        "        for sample in batch:\n",
        "            encoded_circuit = encode_classical_data_optimized(sample.flatten(), quantum_qubits)\n",
        "            full_circuit = quantum_circuit + encoded_circuit\n",
        "            state_vector = extract_reservoir_states_sparse(full_circuit, quantum_qubits)\n",
        "            reservoir_features.append(state_vector.reshape(-1))\n",
        "    return csr_matrix(reservoir_features)\n",
        "\n",
        "reservoir_features_train = process_with_quantum_reservoir_in_batches(X_train, quantum_circuit, quantum_qubits, batch_size=50)\n",
        "del X_train  # Clear training data after processing\n",
        "gc.collect()\n",
        "\n",
        "reservoir_features_test = process_with_quantum_reservoir_in_batches(X_test, quantum_circuit, quantum_qubits, batch_size=50)\n",
        "del X_test  # Clear testing data after processing\n",
        "gc.collect()\n",
        "\n",
        "# Reshape reservoir_features_train and reservoir_features_test to 2D\n",
        "reservoir_features_train = reservoir_features_train.reshape(reservoir_features_train.shape[0], -1)\n",
        "reservoir_features_test = reservoir_features_test.reshape(reservoir_features_test.shape[0], -1)\n",
        "\n",
        "# Train a Logistic Regression Model\n",
        "\n",
        "# Calculate class weights before training\n",
        "classes = np.unique(y_train)\n",
        "class_weights = compute_class_weight(class_weight='balanced', classes=classes, y=y_train)\n",
        "class_weights_dict = dict(zip(classes, class_weights))\n",
        "\n",
        "clf = SGDClassifier(loss=\"log_loss\", class_weight=class_weights_dict) # Change 'loss' to 'log_loss'\n",
        "for i in range(0, reservoir_features_train.shape[0], 100):  # Incremental training\n",
        "    batch_features = reservoir_features_train[i:i+100]\n",
        "    batch_labels = y_train[i:i+100]\n",
        "    clf.partial_fit(batch_features, batch_labels, classes=np.unique(y_train))\n",
        "\n",
        "# Evaluate the Model\n",
        "y_pred_train = clf.predict(reservoir_features_train)\n",
        "y_pred_test = clf.predict(reservoir_features_test)\n",
        "\n",
        "train_accuracy = accuracy_score(y_train, y_pred_train)\n",
        "test_accuracy = accuracy_score(y_test, y_pred_test)\n",
        "\n",
        "# Perform cross-validation\n",
        "scores = cross_val_score(clf, reservoir_features_train, y_train, cv=5, scoring='accuracy')\n",
        "print(f\"Cross-Validation Accuracy Scores: {scores}\")\n",
        "print(f\"Mean Cross-Validation Accuracy: {np.mean(scores) * 100:.2f}%\")\n",
        "\n",
        "print(f\"Train Accuracy: {train_accuracy * 100:.2f}%\")\n",
        "print(f\"Test Accuracy: {test_accuracy * 100:.2f}%\")\n",
        "\n",
        "# Clear reservoir features after model evaluation\n",
        "del reservoir_features_train, reservoir_features_test\n",
        "gc.collect()\n",
        "\n",
        "# Example Predictions\n",
        "print(f\"Predicted Defaults (Test): {y_pred_test[:10]}\")\n",
        "print(f\"Actual Defaults (Test): {y_test[:10]}\") # Remove .values"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#print(X_reduced.explained_variance_)\n",
        "#print(X_reduced.explained_variance_ratio_)\n",
        "#print(X_reduced.explained_variance_ratio_.cumsum())\n",
        "print(pca.explained_variance_)\n",
        "print(pca.explained_variance_ratio_)\n",
        "print(pca.explained_variance_ratio_.cumsum())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vRHzJZdNBtac",
        "outputId": "81d673cd-734e-4104-8a92-34b9e0915a65"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.28685946 0.20729333 0.17171381 0.15225146 0.13713935 0.13020936\n",
            " 0.12331787 0.11255059 0.09129953 0.07951245 0.06870252 0.0574237\n",
            " 0.05389707 0.04970232 0.03894801 0.03252589 0.03046041 0.02768998]\n",
            "[0.15234935 0.11009225 0.09119618 0.08085984 0.07283389 0.06915342\n",
            " 0.06549338 0.05977495 0.04848863 0.04222859 0.0364875  0.03049738\n",
            " 0.02862441 0.02639661 0.02068505 0.01727431 0.01617734 0.01470598]\n",
            "[0.15234935 0.2624416  0.35363778 0.43449763 0.50733152 0.57648493\n",
            " 0.64197832 0.70175327 0.7502419  0.79247049 0.82895799 0.85945537\n",
            " 0.88807979 0.91447639 0.93516145 0.95243575 0.9686131  0.98331908]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results = {'y_pred_train' : y_pred_train,\n",
        "           'y_pred_test' : y_pred_test,\n",
        "           'train_accuracy' : train_accuracy,\n",
        "           'test_accuracy' : test_accuracy}\n",
        "\n",
        "with open('data.pkl', 'wb') as file:\n",
        "    pickle.dump(results, file)"
      ],
      "metadata": {
        "id": "4qKbRfMzGFO1"
      },
      "execution_count": 4,
      "outputs": []
    }
  ]
}